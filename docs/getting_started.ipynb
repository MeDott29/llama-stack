{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Llama Stack !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will walk you throught the steps to get started on LlamaStack\n",
    "The first few steps need to happen outside of this notebook to get a stack server running.\n",
    "Please look at this [guide](https://github.com/meta-llama/llama-stack/blob/main/docs/getting_started.md) for detailed instructions. \n",
    "\n",
    "For more client examples for other apis ( agents, memory, safety ) in llama_stack please refer to the [llama-stack-apps](https://github.com/meta-llama/llama-stack-apps/tree/main/examples).\n",
    "\n",
    "In this notebook, we will showcase a few things to help you get started,\n",
    "- Start the Llama Stack Server \n",
    "- How to use simple text and vision inference llama_stack_client APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the Llama Stack Server "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get Docker container\n",
    "```\n",
    "$ docker login\n",
    "$ docker pull llamastack/llamastack-local-gpu\n",
    "```\n",
    "\n",
    "2. pip install the llama stack client package \n",
    "For this purpose, we will directly work with pre-built docker containers and use the python SDK\n",
    "```\n",
    "$ git clone https://github.com/meta-llama/llama-stack-apps.git\n",
    "$ cd llama-stack-apps\n",
    "$ yes | conda create -n stack-test python=3.10 \n",
    "$ conda activate stack-test\n",
    "$ pip install llama_stack llama_stack_client\n",
    "```\n",
    "This will install `llama_stack` and `llama_stack_client` packages. \n",
    "This will enable you to use the `llama` cli. \n",
    "\n",
    "3. Download model \n",
    "```\n",
    "$ llama download --help \n",
    "$ llama download --source meta --model-id Llama3.2-11B-Vision-Instruct --meta-url <META_URL>\n",
    "```\n",
    "\n",
    "4. Configure the Stack Server\n",
    "```\n",
    "For GPU inference, you need to set these environment variables for specifying local directory containing your model checkpoints, and enable GPU inference to start running docker container.\n",
    "$ export LLAMA_CHECKPOINT_DIR=~/.llama\n",
    "$ llama stack configure llamastack-local-gpu\n",
    "```\n",
    "Follow the prompts as part of configure.\n",
    "Here is a sample output \n",
    "```\n",
    "$ llama stack configure llamastack-local-gpu\n",
    "\n",
    "Could not find llamastack-local-gpu. Trying conda build name instead...\n",
    "Could not find /home/hjshah/.conda/envs/llamastack-llamastack-local-gpu/llamastack-local-gpu-build.yaml. Trying docker image name instead...\n",
    "+ podman run --network host -it -v /home/hjshah/.llama/builds/docker:/app/builds llamastack-local-gpu llama stack configure ./llamastack-build.yaml --output-dir /app/builds\n",
    "\n",
    "Configuring API `inference`...\n",
    "=== Configuring provider `meta-reference` for API inference...\n",
    "Enter value for model (default: Llama3.1-8B-Instruct) (required): Llama3.2-11B-Vision-Instruct\n",
    "Do you want to configure quantization? (y/n): n\n",
    "Enter value for torch_seed (optional): \n",
    "Enter value for max_seq_len (default: 4096) (required): \n",
    "Enter value for max_batch_size (default: 1) (required): \n",
    "\n",
    "Configuring API `safety`...\n",
    "=== Configuring provider `meta-reference` for API safety...\n",
    "Do you want to configure llama_guard_shield? (y/n): n\n",
    "Do you want to configure prompt_guard_shield? (y/n): n\n",
    "\n",
    "Configuring API `agents`...\n",
    "=== Configuring provider `meta-reference` for API agents...\n",
    "Enter `type` for persistence_store (options: redis, sqlite, postgres) (default: sqlite): \n",
    "\n",
    "Configuring SqliteKVStoreConfig:\n",
    "Enter value for namespace (optional): \n",
    "Enter value for db_path (default: /root/.llama/runtime/kvstore.db) (required): \n",
    "\n",
    "Configuring API `memory`...\n",
    "=== Configuring provider `meta-reference` for API memory...\n",
    "> Please enter the supported memory bank type your provider has for memory: vector\n",
    "\n",
    "Configuring API `telemetry`...\n",
    "=== Configuring provider `meta-reference` for API telemetry...\n",
    "\n",
    "> YAML configuration has been written to /app/builds/local-gpu-run.yaml.\n",
    "You can now run `llama stack run local-gpu --port PORT`\n",
    "YAML configuration has been written to /home/hjshah/.llama/builds/docker/local-gpu-run.yaml. You can now run `llama stack run /home/hjshah/.llama/builds/docker/local-gpu-run.yaml`\n",
    "```\n",
    "NOTE: For this example, we use all local meta-reference implementations and have not setup safety. \n",
    "\n",
    "5.  Run the Stack Server\n",
    "```\n",
    "$ llama stack run local-gpu --port 5000\n",
    "```\n",
    "\n",
    "The server has started correctly if you see outputs like the following \n",
    "```\n",
    "...\n",
    "...\n",
    "Listening on :::5000\n",
    "INFO:     Started server process [1]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://[::]:5000 (Press CTRL+C to quit)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama Stack Client examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"192.168.1.15\"\n",
    "port = 5000\n",
    "client = LlamaStackClient(base_url=f\"http://{host}:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this notebook we will be working with the latest Llama3.2 vision models \n",
    "model = \"Llama3.2-1B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference APIs ( chat_completion ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softly padded feet\n",
      "Gentle eyes in ancient soul\n",
      "Mountain's gentle friend"
     ]
    }
   ],
   "source": [
    "# Simple text example \n",
    "iterator = client.inference.chat_completion(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a haiku on llamas\"\n",
    "        }\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in iterator:\n",
    "    print(chunk.event.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multimodal Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import mimetypes \n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# We define a simple utility function to take a local image and \n",
    "# convert it to as base64 encoded data url \n",
    "# that can be passed to the server. \n",
    "def data_url_from_image(file_path):\n",
    "    mime_type, _ = mimetypes.guess_type(file_path)\n",
    "    if mime_type is None:\n",
    "        raise ValueError(\"Could not determine MIME type of the file\")\n",
    "\n",
    "    with open(file_path, \"rb\") as image_file:\n",
    "        encoded_string = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "    data_url = f\"data:{mime_type};base64,{encoded_string}\"\n",
    "    return data_url\n",
    "\n",
    "with open(\"dog.jpg\", \"rb\") as f:\n",
    "    img = Image.open(f).convert(\"RGB\")\n",
    "\n",
    "img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowflakes gently fall\n",
      "Blanketing the winter scene\n",
      "Peaceful winter night"
     ]
    }
   ],
   "source": [
    "# we can reuse the same chat_completion interface for multimodal inference too\n",
    "# Use path to local file\n",
    "data_url = data_url_from_image(\"dog.jpg\")\n",
    "iterator = client.inference.chat_completion(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                { \"image\": { \"uri\": data_url } }, \n",
    "                \"Write a haiku describing the image\"\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in iterator:\n",
    "    print(chunk.event.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
